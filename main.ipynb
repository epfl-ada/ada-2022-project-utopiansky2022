{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6fc6b6",
   "metadata": {},
   "source": [
    "# P2 Code Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb309dd",
   "metadata": {},
   "source": [
    "First, import the necessary libraries and define our data paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaaf5f77",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deplacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15284\\779915011.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdeplacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deplacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import deplacy\n",
    "import os\n",
    "import stanza\n",
    "from thefuzz import process\n",
    "from empath import Empath \n",
    "from CoreNLPanalysis import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "lexicon = Empath()\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dbd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_colab = False\n",
    "if google_colab:\n",
    "    data_folder = '/content/sample_data/'\n",
    "else:\n",
    "    data_folder = './Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d532bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = pd.read_csv(data_folder + \"character.metadata.tsv\", sep='\\t', header = None)\n",
    "characters.columns = ('Wikipedia movie ID', 'Freebase movie ID', 'Movie release date',\n",
    "              'Character name', 'Actor date of birth', 'Actor gender',\n",
    "              'Actor height (in meters)', 'Actor ethnicity (Freebase ID)', 'Actor name', \n",
    "              'Actor age at movie release', 'Freebase character/actor map ID', 'Freebase character ID', \n",
    "              'Freebase actor ID')\n",
    "characters['Movie release date'] = pd.to_datetime(characters['Movie release date'], errors = 'coerce').dt.year\n",
    "\n",
    "# change all formats to datetime and keep the year only\n",
    "# Many of the date formats are inconsistent. \n",
    "# Here we convert it all to just year as this is the temporal resolution in which we are interested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(data_folder + \"movie.metadata.tsv\", sep='\\t', header = None)\n",
    "movies.columns = ('Wikipedia movie ID' , 'Freebase movie ID' , 'Movie name' , \n",
    "                'Movie release date' , 'Movie box office revenue' , \n",
    "                'Movie runtime' , 'Movie languages' , 'Movie countries' , 'Movie genres')\n",
    "\n",
    "movies['Movie release date'] = pd.to_datetime(movies['Movie release date'], errors = 'coerce').dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd18926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading plot summaries\n",
    "df_plot_summaries = pd.read_csv(data_folder + 'plot_summaries.txt', sep='\\t')\n",
    "df_plot_summaries.columns = ['Wikipedia movie ID', 'Plot Summary']\n",
    "print(len(df_plot_summaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c06c5",
   "metadata": {},
   "source": [
    "# Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9088b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9108d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} entries in the movies dataset'.format(len(movies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524dfdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, values in movies.items():\n",
    "    print ('{:.4} % of movies have an associated {}'.format(100 * len(values.loc[values.notnull()])/len(values), name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ef66a",
   "metadata": {},
   "source": [
    "**Note :** Any analysis based on the movie revenue (as a measure of success) will need extra information, as probably basing the study only in 10% of the data is not enough.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3991c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = movies['Movie release date'].hist(bins = range(1880,2030,10), grid = True, xlabelsize = 8 ,xrot = 45)\n",
    "plt.xlabel('Year of Release')\n",
    "plt.ylabel('Number of movies')\n",
    "plt.title('Movies released over the Decades');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7546e7f7",
   "metadata": {},
   "source": [
    "**Note:** On 16 December 2014, Google announced that it would shut down Freebase over the succeeding six months and help with the move of the data from Freebase to Wikidata. Data collection does not go through the entire 2010's decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a91cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataset is comprised of movies with known releases between {0:n} and {1:n} '.format(movies['Movie release date'].min(), \n",
    "                                                                                               movies['Movie release date'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4746df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmovies = movies[movies['Movie box office revenue'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_by_year = rmovies.groupby(\"Movie release date\")['Movie box office revenue'].agg(['mean','size']).reset_index()\n",
    "r_by_year.plot(kind = 'scatter', x = 'Movie release date', y = 'mean',s = 'size')\n",
    "plt.title('Mean Revenue by Year')\n",
    "plt.ylabel('Mean Revenue')\n",
    "plt.xlabel('Year of Release');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f3e57",
   "metadata": {},
   "source": [
    "From the above plot we see that the mean revenues increase over time. It would be interesting to normalize this by inflation or perhaps some share of GDP. We also see that there is much more data present in the later years (dot size scaled by amount of revenues averaged to form datapoint. 1897 - 1 movie revenue, 2008 - 367 movie revenues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed97f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_countries(x):\n",
    "    if len(x)>2:\n",
    "        x= x.split(':')[1][0:-1]\n",
    "        res = x.split('\"')[1]\n",
    "    else:\n",
    "        res = 'NA' \n",
    "    return res\n",
    "\n",
    "movies[\"Movie countries corrected\"]= movies[\"Movie countries\"].apply(lambda x: correct_countries(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20108ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_movies_filt = pd.DataFrame(movies.groupby([\"Movie release date\",\"Movie countries corrected\"]).size()).reset_index()\n",
    "c_movies_filt = c_movies_filt.set_axis([\"Movie release date\", \"Movie countries corrected\", 'Count'], axis='columns', copy=False)\n",
    "c_movies_filt.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba95e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_count = movies.groupby([\"Movie countries corrected\"]).size().reset_index()\n",
    "most_common_count = most_common_count.set_axis([\"Country\", \"Number of movies\"], axis='columns')\n",
    "most_common_count.drop(most_common_count[most_common_count['Country'] == 'NA'].index, inplace = True)\n",
    "most_common_count = most_common_count.sort_values(\"Number of movies\", ascending=False)[0:10]\n",
    "most_common_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f70bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_country_data = c_movies_filt[c_movies_filt['Movie countries corrected'].isin(most_common_count['Country'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='Movie release date', y='Count',data = top10_country_data, hue = 'Movie countries corrected', legend=True)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Numer of times country released movie\")\n",
    "plt.title(\"Release frequency by country over time\")\n",
    "plt.ylim([0,top10_country_data['Count'].max()+50])\n",
    "plt.xlim([top10_country_data['Movie release date'].min(), top10_country_data['Movie release date'].max()]);\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9c407",
   "metadata": {},
   "source": [
    "#### Characters Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf3720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "characters.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} entries in the characters dataset'.format(len(characters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd81472",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, values in characters.items():\n",
    "    print ('{:.4} % of characters have an associated {}'.format(100 * len(values.loc[values.notnull()])/len(values), name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7464fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_filt_age = characters[characters['Actor age at movie release'].notnull()]\n",
    "#we see lots of NA ages at release... lets drop those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba208c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_filt_age.loc[char_filt_age['Actor age at movie release']<0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015d769",
   "metadata": {},
   "source": [
    "**Note:** In the dataframe above we see 381 instances of negative ages. By taking the difference between the actor DOB and movie release date we realize that the actor age is correct in magnitude but not in sign. Below, we correct the column by taking its absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_filt_age.loc[char_filt_age['Actor age at movie release'] < 0, \n",
    "                  'Actor age at movie release'] = char_filt_age['Actor age at movie release'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Only {:.4} % of charactors have an associated actor age at release'.format(100 * len(char_filt_age)/len(characters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_age_grouped = char_filt_age.groupby([\"Movie release date\", \"Actor gender\"])['Actor age at movie release'].agg(['mean','size']).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353853c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we break down the analysis by gender, \n",
    "\n",
    "sns.lmplot(x=\"Movie release date\", y='mean',data = char_age_grouped, hue = 'Actor gender', scatter_kws={\"s\":5})\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average age of Actors\")\n",
    "plt.title(\"Average Age of Actors by Gender and Year of Release\")\n",
    "plt.ylim([0,100])\n",
    "plt.xlim([char_age_grouped[\"Movie release date\"].min(),char_age_grouped[\"Movie release date\"].max()]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e358268",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_age_M = round(char_filt_age.loc[char_filt_age['Actor gender'] == 'M']['Actor age at movie release'].mean(),0)\n",
    "avg_age_F = round(char_filt_age.loc[char_filt_age['Actor gender'] == 'F']['Actor age at movie release'].mean(),0)\n",
    "print('The average age for male actors is ', avg_age_M, ', whereas for actresses it is ', avg_age_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd22fc",
   "metadata": {},
   "source": [
    "In the above figure we are displaying the average age at movie release for female and male actors, as well as the linear regression of the average age evolution over time. The shaded areas represent the 95% confidence interval for the regression.\n",
    "\n",
    "It is interesting to note that males tend to be older than women in cinema over the last hundred years (at least since 1940). This difference observed in the plot is also confirmed by the computed overall average age at realease for both genders (40 for men compared to 33 for women). \n",
    "\n",
    "We can also observe a tendency to employ older actors in movies. One of the hypothesis for this fact is that the age at release data is lacking for older actors at the beginning of the 20th century. Record keeping was not as good as it is now. This could be an explanation for the fact that the data we do have on actors with roles in the early 20th century are all quite young - ie they lived long enough to enter modern database management. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2cd76f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "char_gen = pd.DataFrame(characters.groupby(['Movie release date', 'Actor gender'])['Actor gender'].agg('size')).rename(columns={'Actor gender' : 'Count'}).reset_index()\n",
    "char_gen['Gender Percentage'] = char_gen['Count'] / char_gen.groupby('Movie release date')['Count'].transform('sum')*100\n",
    "char_gen = char_gen.loc[char_gen['Actor gender'] == 'F']\n",
    "char_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7139e5ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x='Movie release date', y='Gender Percentage',data = char_gen, legend=True)\n",
    "l1 = ax.lines[0]\n",
    "x1 = l1.get_xydata()[:, 0]\n",
    "y1 = l1.get_xydata()[:, 1]\n",
    "ax.fill_between(x1, y1, 100, color=\"orange\", alpha=0.2, label = 'Males')\n",
    "\n",
    "ax.fill_between(x1, y1, color=\"blue\", alpha=0.2, label = 'Females')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Percentage of females/males in movies per year\")\n",
    "plt.title(\"Percentage of female and male characters in movies over time\")\n",
    "plt.ylim([0,100])\n",
    "plt.xlim([char_gen['Movie release date'].min(), char_gen['Movie release date'].max()])\n",
    "plt.legend()\n",
    "\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c6d40",
   "metadata": {},
   "source": [
    "We see more dynamic behavior in the earlier years. This is due to small sample sizes which lead to large spikes in % for small deviations. We see it stabilize for a majority of the graph before once again entering erratic territory in the last years (data collection was limited in the 2010's). The most obvious takeaway is that male characters are more common than female characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8013cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking when either character name or Freebase movie ID are missing:\n",
    "print('Missing \"character names\" in character metadata: %d' %characters['Character name'].isnull().sum())\n",
    "print('Missing \"Freebase character\" IDs in character metadata: %d' %characters['Freebase character ID'].isnull().sum())\n",
    "print('We can see that the number of missing character names ≈ missing Freebase character IDs')\n",
    "print('There are 10 characters where we have a \"Freebase character ID\" but not \"character names\"')\n",
    "# Checking when both \"Character name\" and \"Freebase character ID are missing\":\n",
    "characters.loc[characters['Character name'].isnull() & characters['Freebase movie ID'].isnull()]\n",
    "missing_both_IDs = sum(characters.iloc[:, [3, 11]].isnull().all(1))\n",
    "print('Missing Character name AND Freebase character ID: %d' %missing_both_IDs)\n",
    "# Checking characters for which we don't have that character name buit we have both the Freebase character/actor map ID and Freebase actor ID\n",
    "characters.iloc[:, [3, 11]].isnull().all(1) # 4th column and 11th column\n",
    "poss_recov = sum(characters.iloc[:, [3]].isnull().all(1) & characters.iloc[:, [10, 12]].notnull().all(1))\n",
    "print('Missing character name but known Freebase character/actor map ID and Freebase actor ID: %d' %poss_recov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38415e",
   "metadata": {},
   "source": [
    "The previous analysis highlights the possibility of using the available Freebase IDs in order to recover a substantial amount to character names. The Freebase character/actor map will link the Freebase actor ID to the roles that the specific actor will have performed. As a result, we will be able to extract the Freebase character IDs, thus the missing character names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Merging character with movie metadata with an inner join. We do an inner join \n",
    "because there are movies in the movie metadata for which we do not \n",
    "have any character information in character metadata and vice versa.\n",
    "For the future analysis we need both information about the characters\n",
    "and movie in which they appear'''\n",
    "\n",
    "df_char_movie = pd.merge(left= characters, right= movies, how='inner', on= ['Wikipedia movie ID', 'Freebase movie ID', 'Movie release date'])\n",
    "print('The total number of characters that we obtain from the inner merge is: %d' %len(df_char_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3671566",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_with_characters = df_char_movie.groupby(['Wikipedia movie ID']).agg('size')\n",
    "print('The resulting number of movies from the merge of characters and movies is: %d'%len(movies_with_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968610e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the total number of characters and the number of character names we have in our dataframe\n",
    "char_names_per_movie = df_char_movie.groupby('Wikipedia movie ID')['Character name'].count() # extracts number of characters we have (i.e. not coun ting NaNs)\n",
    "total_characters_per_movie = df_char_movie.groupby(['Wikipedia movie ID'])['Character name'].agg(['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f771ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_character_missing_stats = pd.merge(total_characters_per_movie, char_names_per_movie, on='Wikipedia movie ID')\n",
    "df_character_missing_stats['n°_missing_characters'] = df_character_missing_stats['size'] - df_character_missing_stats['Character name']\n",
    "df_character_missing_stats['percentage_missing'] = round(100 * df_character_missing_stats['n°_missing_characters'] / df_character_missing_stats['size'], 2) # computing percentages of missing character names\n",
    "df_character_missing_stats['percentage_available'] = 100 - df_character_missing_stats['percentage_missing']\n",
    "df_character_missing_stats = df_character_missing_stats.rename(columns={'size': 'total_n°_characters (incl. NaN)'})\n",
    "df_character_missing_stats = df_character_missing_stats.rename(columns={'Character name': 'character_names (excl. NaN)'})\n",
    "df_character_missing_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d5d85",
   "metadata": {},
   "source": [
    "In the dataframe above we obtain:\n",
    "- the total amount of characters in a specific movie\n",
    "- the number of character names for a specific movie\n",
    "- the percentage of the character names that are missing per movie\n",
    "- the percentage of the character names that are available per movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_character_missing_stats['percentage_available'].hist(bins=10, grid=False)\n",
    "plt.xticks(np.arange(0, 110, step=10))\n",
    "plt.ylabel('N° movies')\n",
    "plt.xlabel('Percentage of available character names in [%]')\n",
    "plt.title('N° movies according to percentage of known character names')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a71cd8",
   "metadata": {},
   "source": [
    "For example: there are roughly 39 000 movies for which we have less than 10% of the character names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9fce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the percentage of movies for which we have at least 1 character name\n",
    "# Assuming that the character name that would be available is the one of the main character.\n",
    "\n",
    "more_1_char = df_character_missing_stats['character_names (excl. NaN)'][df_character_missing_stats['character_names (excl. NaN)'] >= 1].count()\n",
    "perc_1_char = 100*more_1_char/64329\n",
    "\n",
    "print('The total number of movies where we have at least one character name is: %d' %more_1_char)\n",
    "print('The percentage of movies for which we have at least one character name is: %d' %perc_1_char, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c9c8e",
   "metadata": {},
   "source": [
    "# CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aeac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now we want to proceed to the analysis of the plots using the CoreNLP model. \n",
    "For that we first compare the plot information with the character and movie information, \n",
    "to not analyse plots we can not link to other data. The final dataframe contains only the Wikipedia Modie ID and Plot Summaries\n",
    "which are related to movie and character data'''\n",
    "\n",
    "df_char_movie_plot = pd.merge(left= df_char_movie, right= df_plot_summaries, how='inner', on= ['Wikipedia movie ID'])\n",
    "df_plots_filt = df_char_movie_plot[['Wikipedia movie ID','Plot Summary']].drop_duplicates().reset_index()\n",
    "df_plots_filt = df_plots_filt.drop(columns = ['index'])\n",
    "df_plots_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07cff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('en') # download English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, mwt, pos, lemma, depparse, ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55186727",
   "metadata": {},
   "source": [
    "**Note:** The two following cells should not be ran locally, because of the computational cost. Instead, we used Google Colab with GPU option to run the NLP analysis and create the resulting dataset. Here we keep it for a good understanding of the workflow. The result of the analysis is in the Data folder, and we can import it directly from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b8235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Analyse_Plots function is in a separated python file,\n",
    "#to avoid any information not critical to the understanding of our analysis\n",
    "#It is then imported from CoreNLPanalysis.py\n",
    "df_NLP = Analyse_Plots(df_plots_filt, nlp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce7c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NLP.to_csv(data_folder + \"Plot_NLP_Analysis.csv\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ffd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data obtained from the NLP pipeline:\n",
    "df_1 = pd.read_csv(data_folder + \"Plot_NLP_Analysis_0_14100.csv\", sep='\\t', header = None)\n",
    "df_2 = pd.read_csv(data_folder + \"Plot_NLP_Analysis_14101_23201.csv\", sep='\\t', header = None)\n",
    "df_3 = pd.read_csv(data_folder + \"Plot_NLP_Analysis23202-42302.csv\", sep='\\t', header = None)\n",
    "df_1 = df_1.drop(columns=0)\n",
    "df_2 = df_2.drop(columns=0)\n",
    "df_3 = df_3.drop(columns=0)\n",
    "clusters_df = pd.concat([df_1, df_2, df_3], ignore_index=True)\n",
    "clusters_df.columns = ('Wikipedia movie ID', 'Character partial name', 'Agent Verbs', 'Patient Verbs', 'Attributes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29af5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b2569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_movie_names = df_char_movie[['Wikipedia movie ID','Character name']].dropna()\n",
    "df_char_movie_names['All_names'] = df_char_movie_names.groupby(['Wikipedia movie ID'], \n",
    "            as_index = True)['Character name'].transform(lambda x: ','.join(x))\n",
    "df_char_movie_names = df_char_movie_names[['Wikipedia movie ID', 'All_names']].drop_duplicates()\n",
    "df_char_movie_names['All_names'] = [x.strip('()').split(',') for x in df_char_movie_names['All_names']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3eb6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df = clusters_df.merge(df_char_movie_names, how = 'inner', on = 'Wikipedia movie ID')\n",
    "clusters_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11f006",
   "metadata": {},
   "source": [
    "### Case study example over one movie : \"Bhagwan Dada\", movie ID 10644072 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a02a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first extract the beginning of the plot summary of Bhagwan Dada, movie ID 10644072.\n",
    "\n",
    "text_extract_10644072 = '''A villager new to the big city – Bhagwaan  – turns to crime to avoid starvation. He is drawn into the gang of slum crime lord ‘dada’ Shambu  where he falls into every crime but one: violence against women. When an impoverished but honourable young widow hangs herself after being raped by Shambu Dada, the cries of her orphaned baby awaken Bhagwaan’s soul. He rebels against his former master. Bhagwaan vows to atone for his formerly evil life, and adopts the orphaned baby as his own son. The innocent young boy becomes Bhagwaan’s source of personal redemption and the inspiration for all that he does. 12 years later the man now lovingly hailed as “Bhagwaan Dada” has transformed the former crime-ridden slum into a safe and happy neighbourhood - Shantinagar - where his adopted son Govinda  has become the pride and joy of the whole community. Despite his own dark past, this loving father has worked hard to raise the boy as a good person with sound moral values. At this time, Bhagwaan chances to rescue another naïve young man new to the big city – Swaroop  – when Shambu Dada’s gang steal all Swaroop’s money. Impressed with Swaroop’s education as well as the similarity of situation to his own arrival in the city years before, Bhagwaan takes the “innocent and simple” villager under his protective wing. He arranges a good job, even brings Swaroop home to live with himself and his son, young Govinda. Swaroop and Bhagwaan claim each other as ‘brothers’.'''\n",
    "text_extract_10644072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e656c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we run the pipeline over the plot summary which will run annotation over each sentence of the plot:\n",
    "doc_10644072 = nlp(text_extract_10644072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475fe96a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Then, we can have a look at the dependencies of the words between each other:\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc_10644072.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b05235",
   "metadata": {},
   "source": [
    "As we can see, the result of these show each tokenized word of the sentence, their id, and their dependency relationship with other words and their ids in the same sentence. The deprel type relies on the Stanford typed dependencies representation \"which was designed to provide a simple description of the grammatical relationships in a sentence that can easily be understood and effectively used by people\n",
    "without linguistic expertise who want to extract textual relations\" (Marneffe and Manning, 2008)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa6395",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#We can visualize the results of these annotations with the first sentences of the plot summary with the deplacy library:\n",
    "deplacy.render(doc_10644072)\n",
    "#Here, it also shows the position of the word, meaning the type of word it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another type of visualization, maybe more visual, with port=None to be visualized directly within the notebook is the following:\n",
    "deplacy.serve(doc_10644072, port=None, RtoL=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ea3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, we select the plot summary within all the plots, to demonstrate a typical output we get from our pipeline:\n",
    "plot_10644072 = df_plots_filt[df_plots_filt['Wikipedia movie ID']==10644072].reset_index()\n",
    "plot_10644072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a lot of time to run (4 hours or more without google colab GPU) if you don't select a subset of plot summaries:\n",
    "# Here, we select the movie ID of our example and it returns us a dataframe with all the characters (entities) found in the plot summary\n",
    "# with their corresponding agent verbs, patient verbs and attributes:\n",
    "df_NLP = Analyse_Plots(plot_10644072, nlp)  \n",
    "df_NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d145412",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "for elem in ['Shambu', 'Bhagwaan', 'Bijli','Madhu']:\n",
    "    print(elem,'\\'s attributes in the movie plot correspond to :', df_NLP.loc[df_NLP['Character_Name']==elem]['Attributes'].values)\n",
    "    print(elem,'\\'s agent verbs, meaning verbs for which he\\'s taking the action in the movie plot correspond to :', df_NLP.loc[df_NLP['Character_Name']==elem]['Agent Verbs'].values)\n",
    "    print(elem,'\\'s patient verbs, meaning verbs for which he is the action\\'s subject in the movie plot correspond to :', df_NLP.loc[df_NLP['Character_Name']==elem]['Patient Verbs'].values)\n",
    "    print('    ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a43a8",
   "metadata": {},
   "source": [
    "Here, we can see that it depicts a quite satisfying big picture of the character, where we can later analyse the lexical field of the words employed in the plot summary to describe him to define the archetype of this character. We can clearly retain from our analysis that Shambu holds a negative and violent description of himself depicted as a fugitive and evil master of the gang Bhagwaan was involved into. Bhagwaan on the contrary is correctly defined in the opposite way in our analysis by his 'soul' and thirst of revenge against his former master. Of course, only the archetype can be deduced from our list of attributes and verbs and not the meaning and relation between the characters, but this will help us to do archetypes comparison between character's genders later in the analysis. Indeed, Bijili and Madhu, both feminine characters, have much less verbs and attributes compared to previous masculine characters and their corresponding verbs are more on the 'victim' side: manages, confesses, help, as well as their attributes: sex worker, attraction, streetwalker etc. Of course, this concerns only one movie and cannot be generalized yet but we are confident about our results and can now move on to merge our outputs of the NLP pipeline with corresponding character names and movie IDs from character and movie metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a1ed9",
   "metadata": {},
   "source": [
    "**Note:** Same as previously, we will not run those following cells, but they are useful to understand where our data comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(col1, col2):\n",
    "    if process.extractOne(col1, list(col2))[1]>80:\n",
    "        res = process.extractOne(col1, list(col2))[0]    \n",
    "    else:\n",
    "        res = 'Not found'\n",
    "    return res\n",
    "\n",
    "clusters_df['Full_name'] = clusters_df.apply(lambda x: matching(x['Character partial name'],\n",
    "                                                                            x['All_names']), axis=1)\n",
    "clusters_df.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_df = clusters_df.loc[clusters_df['Full_name'] != 'Not found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efd607",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Agent Verbs', 'Patient Verbs', 'Attributes']\n",
    "\n",
    "for i,cat in enumerate(categories):\n",
    "    rem = char_df[cat] == '0.0'\n",
    "    char_df.loc[rem, cat] = char_df.loc[rem, cat].replace('0.0', '')\n",
    "char_df['All_Agent_Verbs'] = char_df.groupby(['Full_name'])['Agent Verbs'].transform(lambda x: ','.join(x))\n",
    "char_df['All_Patient_Verbs'] = char_df.groupby(['Full_name'])['Patient Verbs'].transform(lambda x: ','.join(x))\n",
    "char_df['All_Attributes'] = char_df.groupby(['Full_name'])['Attributes'].transform(lambda x: ','.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b5e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_df = char_df[['Wikipedia movie ID', 'Full_name', 'All_Agent_Verbs', 'All_Patient_Verbs', 'All_Attributes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_df.to_csv(data_folder + \"Characters_Matched_DF_Clean.csv\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9981a69f",
   "metadata": {},
   "source": [
    "## 3- Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97180881",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMdf = pd.read_csv('Data/Characters_Matched_DF_Clean.csv', sep='\\t',index_col = 0)\n",
    "CMdf.columns = ['Wikipedia movie ID', 'Character name', 'Agent Verbs', 'Patient Verbs', 'Attributes']\n",
    "CMdf.reset_index(inplace = True,drop=True)\n",
    "CMdf.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMdf = pd.merge(left= CMdf, right=df_char_movie, how='inner', on= ['Wikipedia movie ID', 'Character name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79371250",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMdf_c = CMdf[['Wikipedia movie ID','Character name','Actor gender','Agent Verbs','Patient Verbs','Attributes']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6a178",
   "metadata": {},
   "source": [
    "## Columns kept:\n",
    "In this section, we extract the following columns for each character:\n",
    "We keep only features such as for the preliminary sentiment analysis: \n",
    "- `Wikipedia movie ID`: ID of the movie from wikidata\n",
    "- `Character name`: character name associated with verbs and attributes\n",
    "- `Actor gender`\n",
    "- `Agent Verbs`\n",
    "- `Patient Verbs`\n",
    "- `Attributes`\n",
    "\n",
    "However, for later analysis, we would want to add other interesting features such as:\n",
    "- `Movie release date`: the release date of the movie the character is from\n",
    "- `nbr_words`: median of number of words in the plot summary corresponding to the movie ID\n",
    "- `nbr_characters`: median of number of characters present in the movie (that we have in the final data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our lexicon.analyze requires a string of words with a space between them. \n",
    "# to transform current AV,PV, Attributes into required format: \n",
    "\n",
    "cols = ['Agent Verbs', 'Patient Verbs', 'Attributes']\n",
    "\n",
    "for i in cols:\n",
    "    CMdf_c[i] = CMdf_c[i].str.replace(',' ,' ')\n",
    "    CMdf_c[i] = CMdf_c[i].str.replace('[' ,'')\n",
    "    CMdf_c[i] = CMdf_c[i].str.replace(']' ,'')\n",
    "    CMdf_c[i] = CMdf_c[i].str.replace(\"'\" ,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMdf_c.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962517af",
   "metadata": {},
   "source": [
    "## Feature extraction:\n",
    "-  `scores` returned by the `analyze` function of the `Empath library` for different lexical categories. We choose these categories to cover a wide range of lexical fields for the purpose of our data analysis and obtain a score for each one of them. The whole list of categories of this library can be found following this link: https://github.com/Ejhfast/empath-client/blob/master/empath/data/categories.tsv.\n",
    "- `Richness of the vocabulary`, that we compute by counting the median number of different words in the attributes, agent and patient verbs for women and men.\n",
    "Other informations:\n",
    "For later:\n",
    "- `Top10Words`: presence or absence of verbs or attributes in the top 10 words occurring the most (make categories of them ? such as top10 violence, top10 sex etc)\n",
    "\n",
    "These features could be added to the columns of the previous dataframe for each character but this would render the analysis too difficult for more than 15 000 characters !! For this reason, we first group the data in 2 groups: men and women, for a preliminary sentiment analysis but we want to extend this to common archetypes by gender later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d50e4",
   "metadata": {},
   "source": [
    "### 3.1  Lexical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86523c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping of the data by gender\n",
    "# then we randomly mis data to create several subsets for male and female\n",
    "# each subset will be a sample for the PCA analysis\n",
    "women = CMdf_c.loc[CMdf_c['Actor gender'] == 'F']\n",
    "men = CMdf_c.loc[CMdf_c['Actor gender'] == 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "women.fillna('', inplace=True)\n",
    "men.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab748341",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in ['Agent Verbs', 'Patient Verbs', 'Attributes']:\n",
    "\n",
    "    women[elem] = women[elem].astype(\"string\")\n",
    "    men[elem] = men[elem].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e5d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The cleaned and processed lexical information contains {} male movie characters and {} female movie characters'.format(len(men),len(women)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabefff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "men_agent_verbs = men['Agent Verbs'].str.split(expand=True).stack().value_counts().to_frame()\n",
    "men_agent_verbs = men_agent_verbs.reset_index(level=0)\n",
    "men_agent_verbs.set_axis(['agent_verb','verb_count'], axis='columns', inplace=True)\n",
    "men_agent_verbs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "women_agent_verbs = women['Agent Verbs'].str.split(expand=True).stack().value_counts().to_frame()\n",
    "women_agent_verbs = women_agent_verbs.reset_index(level=0)\n",
    "women_agent_verbs.set_axis(['agent_verb','verb_count'], axis='columns', inplace=True)\n",
    "women_agent_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe398f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "men_patient_verbs = men['Patient Verbs'].str.split(expand=True).stack().value_counts().to_frame()\n",
    "men_patient_verbs = men_patient_verbs.reset_index(level=0)\n",
    "men_patient_verbs.set_axis(['patient_verb','verb_count'], axis='columns', inplace=True)\n",
    "men_patient_verbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d10c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "women_patient_verbs = women['Patient Verbs'].str.split(expand=True).stack().value_counts().to_frame()\n",
    "women_patient_verbs = women_patient_verbs.reset_index(level=0)\n",
    "women_patient_verbs.set_axis(['patient_verb','verb_count'], axis='columns', inplace=True)\n",
    "women_patient_verbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "men_attributes = men['Attributes'].str.split(expand=True).stack().value_counts().to_frame()\n",
    "men_attributes = men_attributes.reset_index(level=0)\n",
    "men_attributes.set_axis(['attributes','attribute_count'], axis='columns', inplace=True)\n",
    "men_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "women_attributes = women['Attributes'].str.split(expand=True).stack().value_counts()\n",
    "women_attributes = women_attributes.reset_index(level=0)\n",
    "women_attributes.set_axis(['attributes','attribute_count'], axis='columns', inplace=True)\n",
    "women_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc0db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe shows the top agent verbs for men and women above a certain threshold (for either of the genders)\n",
    "# The values have been normalised by the total number of words for each gender.\n",
    "\n",
    "common_agent_verbs = pd.merge(left= men_agent_verbs, right=women_agent_verbs, how='inner', on= ['agent_verb'])\n",
    "common_agent_verbs.set_axis(['agent_verb','verb_count_men', 'verb_count_women'], axis='columns', inplace=True)\n",
    "\n",
    "total_verbs_men = sum(common_agent_verbs['verb_count_men'])\n",
    "total_verbs_women = sum(common_agent_verbs['verb_count_women'])\n",
    "\n",
    "common_agent_verbs['verb_count_men'] = 1000*common_agent_verbs['verb_count_men']/total_verbs_men\n",
    "common_agent_verbs['verb_count_women'] = 1000*common_agent_verbs['verb_count_women']/total_verbs_women\n",
    "\n",
    "common_agent_verbs = common_agent_verbs.loc[(common_agent_verbs['verb_count_men']>=10) | (common_agent_verbs['verb_count_women']>=10)]\n",
    "\n",
    "common_agent_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4befcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe shows the top patient verbs for men and women above a certain threshold (for either of the genders)\n",
    "# The values have been normalised by the total number of words for each gender.\n",
    "\n",
    "common_patient_verbs = pd.merge(left= men_patient_verbs, right=women_patient_verbs, how='inner', on= ['patient_verb'])\n",
    "common_patient_verbs.set_axis(['agent_verb','verb_count_men', 'verb_count_women'], axis='columns', inplace=True)\n",
    "\n",
    "total_patient_verbs_men = sum(common_patient_verbs['verb_count_men'])\n",
    "total_patient_verbs_women = sum(common_patient_verbs['verb_count_women'])\n",
    "\n",
    "common_patient_verbs['verb_count_men'] = 1000*common_patient_verbs['verb_count_men']/total_patient_verbs_men\n",
    "common_patient_verbs['verb_count_women'] = 1000*common_patient_verbs['verb_count_women']/total_patient_verbs_women\n",
    "\n",
    "common_patient_verbs = common_patient_verbs.loc[(common_patient_verbs['verb_count_men']>=10) | (common_patient_verbs['verb_count_women']>=10)]\n",
    "\n",
    "common_patient_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81781090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This dataframe shows the top attributes for men and women above a certain threshold (for either of the genders)\n",
    "# The values have been normalised by the total number of words for each gender.\n",
    "\n",
    "common_attributes = pd.merge(left= men_attributes, right=women_attributes, how='inner', on= ['attributes'])\n",
    "common_attributes.set_axis(['attributes','attribute_count_men', 'attribute_count_women'], axis='columns', inplace=True)\n",
    "\n",
    "total_attributes_men = sum(common_attributes['attribute_count_men'])\n",
    "total_attributes_women = sum(common_attributes['attribute_count_women'])\n",
    "\n",
    "common_attributes['attribute_count_men'] = 1000*common_attributes['attribute_count_men']/total_attributes_men\n",
    "common_attributes['attribute_count_women'] = 1000*common_attributes['attribute_count_women']/total_attributes_women\n",
    "\n",
    "common_attributes = common_attributes.loc[(common_attributes['attribute_count_men']>=10) | (common_attributes['attribute_count_women']>=10)]\n",
    "\n",
    "common_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc76ccce",
   "metadata": {},
   "source": [
    "### 3.2 Empath Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b79a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "women_lex_av = pd.DataFrame([lexicon.analyze(women['Agent Verbs'].str.cat(sep = ' '))])\n",
    "women_lex_pv = pd.DataFrame([lexicon.analyze(women['Patient Verbs'].str.cat(sep = ' '))])\n",
    "women_lex_att = pd.DataFrame([lexicon.analyze(women['Attributes'].str.cat(sep = ' '))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695162c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "men_lex_av = pd.DataFrame([lexicon.analyze(men['Agent Verbs'].str.cat(sep = ' '))])\n",
    "men_lex_pv = pd.DataFrame([lexicon.analyze(men['Patient Verbs'].str.cat(sep = ' '))])\n",
    "men_lex_att = pd.DataFrame([lexicon.analyze(men['Attributes'].str.cat(sep = ' '))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952db8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_lex = pd.concat([women_lex_av, women_lex_pv, women_lex_att, men_lex_av, men_lex_pv, men_lex_att], ignore_index=True)\n",
    "gendered_lex.rename(index={0:\"Agent Verb [F]\", 1:\"Patient Verb [F]\", 2: \"Attribute [F]\" ,3:\"Agent Verb [M]\", 4:\"Patient Verb [M]\", 5: \"Attribute [M]\"}, inplace = True)\n",
    "gendered_lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ce2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_lex['Total words'] = gendered_lex.sum(axis=1)\n",
    "gendered_lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc059ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_lex['Total words'] = gendered_lex.sum(axis=1)\n",
    "gendered_lex\n",
    "for col in gendered_lex.columns:\n",
    "    gendered_lex[col] = gendered_lex[col]/gendered_lex['Total words']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c48fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gendered_lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ae0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c0ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3fce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_chars = women.sample(frac = 1, random_state = 1).reset_index()\n",
    "male_chars = men.sample(frac = 1, random_state = 1).reset_index()\n",
    "idx_f = np.arange(0, len(female_chars),500)\n",
    "samples_f = dict()\n",
    "for i in range(len(idx_f)-1):\n",
    "    samples_f['df_f_'+str(i)] = female_chars.iloc[idx_f[i]:idx_f[i+1],:]\n",
    "idx_m = np.arange(0, len(male_chars),500)\n",
    "samples_m = dict()\n",
    "for i in range(len(idx_m)-1):\n",
    "    samples_m['df_m_'+str(i)] = male_chars.iloc[idx_m[i]:idx_m[i+1],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc(x):\n",
    "    if len(x)==0:\n",
    "        res = 0\n",
    "    else:\n",
    "        res = len(x.split())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_num_av_f = []\n",
    "med_num_pv_f = []\n",
    "med_num_att_f = []\n",
    "\n",
    "for i in samples_f.keys():\n",
    "    times_av = samples_f[i]['Agent Verbs'].apply(lambda x: pd.Series({'sum': voc(x)}))\n",
    "    med_num_av_f.append(times_av['sum'].median())\n",
    "    times_pv = samples_f[i]['Patient Verbs'].apply(lambda x: pd.Series({'sum': voc(x)}))\n",
    "    med_num_pv_f.append(times_pv['sum'].median())\n",
    "    times_att = samples_f[i]['Attributes'].apply(lambda x: pd.Series({'sum': voc(x)}))\n",
    "    med_num_att_f.append(times_att['sum'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a62aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_num_av_m = []\n",
    "med_num_pv_m = []\n",
    "med_num_att_m = []\n",
    "\n",
    "for i in samples_m.keys():\n",
    "    times_av = samples_m[i]['Agent Verbs'].apply(lambda x: pd.Series({'sum': voc(x)}))\n",
    "    med_num_av_m.append(times_av['sum'].median())\n",
    "    times_pv = samples_m[i]['Patient Verbs'].apply(lambda x: pd.Series({'sum': voc(x)}))\n",
    "    med_num_pv_m.append(times_pv['sum'].median())\n",
    "    times_att = samples_m[i]['Attributes'].apply(lambda x: pd.Series({'sum': voc(x)}))\n",
    "    med_num_att_m.append(times_att['sum'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51860f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['help','money','wedding','domestic_work','hate','cheerfulness','aggression','occupation','envy','anticipation','family','crime','attractive','masculine','prison'\n",
    ",'pride','dispute','nervousness','weakness','government','horror','swearing_terms','suffering','wealthy','royalty','magic','school','beach','banking','social_media'\n",
    ",'exercise','night','kill','blue_collar_job','art','ridicule','play','computer','optimism','stealing','home','sexual','fear','irritability','superhero'\n",
    ",'business','driving','childish','cooking','exasperation','religion','surprise','reading','worship','leader','independence','movement','body','zest','confusion','sports','death','healing','legend','heroic','celebration'\n",
    ",'violence','dominant_hierarchical','military','neglect','exotic','love','communication','hearing','order','sympathy','anonymity','trust','ancient','deception'\n",
    ",'fight','dominant_personality','politeness','farming','meeting','war','speaking','listen','shopping','disgust','tool','gain','injury','rage','science','work','appearance','valuable','warmth','youth','sadness','fun','emotional','joy','affection','fashion','ugliness'\n",
    "'lust','shame','torment','economics','anger','politics','strength','breaking','shape_and_size','power','white_collar_job','terrorism','party','disappointment','poor','pain','beauty','timidity','philosophy','negotiate','negative_emotion','cleaning','competing','law','friends','achievement','alcohol'\n",
    ",'feminine','weapon','children','monster','giving','contentment','positive_emotion','writing','rural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Agent Verbs', 'Patient Verbs', 'Attributes']\n",
    "samples = [samples_f, samples_m]\n",
    "for df in samples:\n",
    "    for key in df.keys():\n",
    "        df[key]['Descriptors'] = df[key][cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8391257",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_analysis_f = {}\n",
    "sent_analysis_m= {}\n",
    "\n",
    "for i, key in enumerate(samples_f.keys()):\n",
    "    sent_analysis_f['df_f_'+str(i)] = pd.DataFrame([lexicon.analyze(samples_f[key]['Descriptors'].str.cat(sep = ' '), categories = categories)])\n",
    "    \n",
    "for i, key in enumerate(samples_m.keys()):\n",
    "    sent_analysis_m['df_m_'+str(i)] = pd.DataFrame([lexicon.analyze(samples_m[key]['Descriptors'].str.cat(sep = ' '), categories = categories)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e65f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_fem = pd.concat(sent_analysis_f.values(), ignore_index=True)\n",
    "feats_male = pd.concat(sent_analysis_m.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ebcbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_fem['Total words'] = feats_fem.sum(axis=1)\n",
    "for col in feats_fem.columns:\n",
    "    feats_fem[col] = feats_fem[col]/feats_fem['Total words']\n",
    "\n",
    "feats_male['Total words'] = feats_male.sum(axis=1)\n",
    "for col in feats_male.columns:\n",
    "    feats_male[col] = feats_male[col]/feats_male['Total words']\n",
    "    \n",
    "feats_fem = feats_fem.drop(columns = ['Total words'])\n",
    "feats_male = feats_male.drop(columns = ['Total words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_fem['med_num_av'] = med_num_av_f\n",
    "feats_fem['med_num_pv'] = med_num_pv_f\n",
    "feats_fem['med_num_att'] = med_num_att_f\n",
    "feats_fem['gender'] = 0\n",
    "feats_male['med_num_av'] = med_num_av_m\n",
    "feats_male['med_num_pv'] = med_num_pv_m\n",
    "feats_male['med_num_att'] = med_num_att_m\n",
    "feats_male['gender'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be424fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([feats_fem, feats_male], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f57624",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns = 'gender')\n",
    "y = data['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(X), columns = X.columns).copy()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pca_screeplot = PCA(n_components=2)\n",
    "pca_screeplot.fit_transform(scaled_df)\n",
    "pca = PCA().fit(scaled_df)\n",
    "\n",
    "# Percentage variance explaines\n",
    "ratio = pca_screeplot.explained_variance_ratio_\n",
    "print('The 2D PCA contains',round((ratio[0]+ratio[1])*100,4),'% of the information' )\n",
    "print(\"\")\n",
    "print ('Component 1 contains',ratio[0]*100,'% of the information' )\n",
    "print ('Component 2 contains',ratio[1]*100,'% of the information' )\n",
    "print(\"\")\n",
    "print ('So far,',(1-np.sum([ratio[0],ratio[1]]))*100,'% of the information has been lost' )\n",
    "\n",
    "#Scree Plot\n",
    "plt.ylabel(\"Eigenvalues\")\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.title(\"Scree plot for our PCA\")\n",
    "plt.axhline(y = 1, color = 'r', linestyle = '--')\n",
    "plt.plot(pca.explained_variance_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3060425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_2d.fit_transform(scaled_df)\n",
    "pca = PCA().fit(scaled_df)\n",
    "\n",
    "map_ = pd.DataFrame(pca_2d.components_,columns=scaled_df.columns)\n",
    "sns.heatmap(map_,cmap=\"PiYG\")\n",
    "plt.title(\"Features importance in Quotebank speakers PCA\")\n",
    "plt.yticks([0,1],['Component 1','Component 2'], fontsize=10)\n",
    "plt.xlabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0525c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D PCA\n",
    "PCA_2d = PCA(n_components=2)\n",
    "PCs_2 = PCA_2d.fit_transform(scaled_df)\n",
    "res_PCA_2d = pd.DataFrame(data = PCs_2, columns = ['Principal component 1', 'Principal component 2'])\n",
    "\n",
    "#3D PCA\n",
    "PCA_3d = PCA(n_components=3)\n",
    "PCs_3 = PCA_3d.fit_transform(scaled_df)\n",
    "res_PCA_3d = pd.DataFrame(data = PCs_3, columns = ['Principal component 1', 'Principal component 2', 'Principal component 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize results 2D PCA\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlabel('Principal Component 1',fontsize = 15)  \n",
    "plt.ylabel('Principal Component 2',fontsize = 15)  \n",
    "plt.title(\"PCA with 2 Components on gendered attributes, patient and agent verbs\",fontsize=20) \n",
    "\n",
    "plot = plt.scatter(res_PCA_3d.loc[:,'Principal component 1'], res_PCA_3d.loc[:,'Principal component 2'], c = pca_input['gender'])\n",
    "plt.legend(handles=plot.legend_elements()[0], labels=list(pca_input['gender']))\n",
    "\n",
    "plt.grid()\n",
    "#plt.savefig(\"data/PCA_visualization.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559887d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = PCA_2d.components_.T * np.sqrt(PCA_2d.explained_variance_)\n",
    "loading_matrix = pd.DataFrame(abs(loadings), columns=['PC1', 'PC2'], index=X.columns)\n",
    "loading_matrix = loading_matrix.sort_values('PC1', ascending = False)\n",
    "loading_matrix = loading_matrix.loc[loading_matrix['PC1']>0.8]\n",
    "loading_matrix.drop(columns = ['PC2'], axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
