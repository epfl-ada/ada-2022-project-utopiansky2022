{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adf8fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stanza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd3954f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 17:28:21 WARNING: Directory C:/Users/nerea/CoreNLP already exists. Please install CoreNLP to a new directory.\n"
     ]
    }
   ],
   "source": [
    "stanza.install_corenlp(dir=\"C:/Users/nerea/CoreNLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bbef36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c7f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = \"C:/Users/nerea/CoreNLP\"\n",
    "os.environ[\"_JAVA_OPTIONS\"] = '-Xmx512M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
    "ann = client.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112800c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03fc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddff6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59058d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' Use the ner processor's output to get the \"PERSONS\" in the plots\n",
    "We take only the first name'''\n",
    "\n",
    "def get_characters(doc):\n",
    "    characters = []\n",
    "    characters_name = []\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.ents:\n",
    "            if word.type == 'PERSON' and word.text not in characters:\n",
    "                characters.append([word.text])\n",
    "                characters_name.append([word.text.split(' ')[0]])\n",
    "    characters = list(np.unique(characters))\n",
    "    characters_name = list(np.unique(characters_name))\n",
    "    return characters, characters_name\n",
    "\n",
    "'''For each character, we look at immediate verb governors and attribute syntactic dependencies to all of the entity’s mention headwords that are extracted from the typed dependency tuples produced by the parser:\n",
    "    + Agent verbs. Verbs for which the entity is an agent argument (nsubj or agent).\n",
    "    + Patient verbs. Verbs for which the entity is the patient, theme or other argument (dobj, nsubjpass, iobj, or any prepositional argument prep *).\n",
    "    + Attributes. Adjectives and common noun words that relate to the mention as adjectival modifiers, noun-noun compounds, appositives, or copulas (nsubj or appos governors, or nsubj, appos, amod, nn dependents of an entity mention). \n",
    "    \n",
    "    We end up with a dataframe containing Agent Verbs, Patient Verbs and Attributes corresponding to each character in the plot. '''\n",
    "\n",
    "''' This function finds attributes recursively, by first checking the words in \n",
    "the sentence which are not roots (main verb), and then checking all adjectives\n",
    "and conjunctions related to those words'''\n",
    "\n",
    "def recursive_find_adjs(root, sentence):\n",
    "    children = [w for w in sentence.words if w.head == root.id]\n",
    "    if not children:\n",
    "        pass \n",
    "    filtered_child = [w for w in children if (w.deprel == \"conj\" or w.deprel == \"compound\" or w.deprel == \"nsubj\") and (w.pos == \"ADJ\"or w.pos == 'NOUN' or adj0.pos == \"ADV\" or adj0.pos == \"CCONJ\"or adj0.pos == \"AUX\" or adj0.pos == \"ADP\")] #or w.pos == 'NOUN'\n",
    "    results = [w for w in filtered_child if not any(sub.head == w.id and sub.upos == \"NOUN\" for sub in sentence.words)]\n",
    "    for w in children:\n",
    "        results += recursive_find_adjs(w, sentence)\n",
    "    return results\n",
    "\n",
    "''' The following function uses the recursive search of attributes and outputs a dataframe with the character name and its attributes'''\n",
    "\n",
    "def char_attributes(doc):\n",
    "    names = []\n",
    "    names_2 = []\n",
    "    attributes = []\n",
    "    attributes_2 = []\n",
    "    for sent in doc.sentences:\n",
    "        nouns = [w for w in sent.words if w.pos == \"PROPN\"]\n",
    "        for noun in nouns:\n",
    "            if noun.text in get_characters(doc)[1]:\n",
    "                # Find constructions in the form of \"The car is beautiful\"\n",
    "                # In this scenario, the adjective is the parent of the noun\n",
    "                adj0 = sent.words[noun.head-1] #adjective directly related\n",
    "                adjs = [adj0] + recursive_find_adjs(adj0, sent) if adj0.pos == \"ADJ\" or adj0.pos == \"NOUN\" or adj0.pos == \"ADV\" or adj0.pos == \"CCONJ\"or adj0.pos == \"AUX\" or adj0.pos == \"ADP\" else []               \n",
    "                #The recursive function finds adjectives related to the first one found,\n",
    "                #and hence also linked to the target noun\n",
    "                mod_adjs = [w for w in sent.words if w.head == noun.id and (w.pos == \"ADJ\")]\n",
    "                # This should only be one element because conjunctions are hierarchical\n",
    "                if mod_adjs:\n",
    "                    mod_adj = mod_adjs[0]\n",
    "                    adjs.extend([mod_adj] + recursive_find_adjs(mod_adj, sent))\n",
    "                if adjs:\n",
    "                    unique_adjs = []\n",
    "                    unique_ids = set()\n",
    "                    for adj in adjs:\n",
    "                        if adj.id not in unique_ids:\n",
    "                            unique_adjs.append(adj)\n",
    "                            unique_ids.add(adj.id)\n",
    "                    names.append(noun.text)\n",
    "                    attributes.append(\" \".join([adj.text for adj in unique_adjs]))\n",
    "    char_attributes = pd.DataFrame()\n",
    "    char_attributes['Character Names'] = names\n",
    "    char_attributes['Character Attributes'] = attributes\n",
    "    char_attributes['Total Attributes'] = char_attributes.groupby('Character Names')['Character Attributes'].transform(lambda x: ' '.join(x))\n",
    "    char_attributes= char_attributes[['Character Names','Total Attributes']]\n",
    "    return (char_attributes.drop_duplicates().reset_index())\n",
    "\n",
    "''' This function finds agent and patient verbs using the deprel output of the \n",
    "depparse processor'''\n",
    "\n",
    "def agent_patient_verbs(doc):\n",
    "    agent_verbs = {'id': [], 'word': [], 'head_id': [], 'agent_verbs': []}\n",
    "    patient_verbs = {'id': [], 'word': [], 'head_id': [], 'patient_verbs': []}\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.deprel == \"nsubj\" or word.deprel == \"acl:relcl\":\n",
    "                agent_verbs['id'].append(word.id)\n",
    "                agent_verbs['word'].append(word.text)\n",
    "                agent_verbs['head_id'].append(word.head)\n",
    "                agent_verbs['agent_verbs'].append(sentence.words[word.head-1].text)\n",
    "            elif word.deprel == \"nsubj:pass\" or word.deprel == \"dobj\" or word.deprel == \"iobj\":\n",
    "                patient_verbs['id'].append(word.id)\n",
    "                patient_verbs['word'].append(word.text)\n",
    "                patient_verbs['head_id'].append(word.head)\n",
    "                patient_verbs['patient_verbs'].append(sentence.words[word.head-1].text)\n",
    "\n",
    "    return (pd.DataFrame(data=agent_verbs), pd.DataFrame(data=patient_verbs))\n",
    "\n",
    "\n",
    "''' Here we implement the NLP analysis, using the previous functions to get the verbs and attributes related to the found characters in a plot summary'''\n",
    "\n",
    "\n",
    "def create_table_dependencies(plot, nlp):\n",
    "    doc = nlp(plot)\n",
    "    attrs_table = char_attributes(doc)\n",
    "    agent_verbs = agent_patient_verbs(doc)[0] \n",
    "    patient_verbs = agent_patient_verbs(doc)[1] \n",
    "    attrs_table['Agent Verbs'] = np.zeros(len(attrs_table['Character Names']))\n",
    "    attrs_table['Patient Verbs'] = np.zeros(len(attrs_table['Character Names']))\n",
    "    for idx, char in enumerate(attrs_table['Character Names']):\n",
    "        av = []\n",
    "        for idx2, w in enumerate(agent_verbs['word']):\n",
    "            if (w in attrs_table['Total Attributes'][idx] or w == char):\n",
    "                av.append(agent_verbs['agent_verbs'][idx2])\n",
    "                attrs_table['Agent Verbs'][idx] = av\n",
    "        pv = []\n",
    "        for idx2, w in enumerate(patient_verbs['word']):\n",
    "            if (w in attrs_table['Total Attributes'][idx] or w == char):\n",
    "                pv.append(patient_verbs['patient_verbs'][idx2])\n",
    "                attrs_table['Patient Verbs'][idx] = pv\n",
    "            \n",
    "    return attrs_table\n",
    "\n",
    "''' Here is the main function, which loops through the whole dataset and creates a new one containing movie IDs, character's first name, attributes, agent verbs and patient verbs'''\n",
    "\n",
    "def Analyse_Plots(df_plots, nlp):\n",
    "    plot_analysis = pd.DataFrame()\n",
    "    chars = []\n",
    "    movies = []\n",
    "    averbs = []\n",
    "    pverbs = []\n",
    "    attrs = []\n",
    "    for i, summ in enumerate(df_plots['Plot Summary']):\n",
    "        print('Plot analysed ', i, ' out of ', len(df_plots['Plot Summary']))\n",
    "        male_gaze = create_table_dependencies(summ, nlp)\n",
    "        for j in range(len(male_gaze)):\n",
    "            movies.append(df_plots['Wikipedia movie ID'][i])\n",
    "            chars.append(male_gaze['Character Names'][j])\n",
    "            averbs.append(male_gaze['Agent Verbs'][j])\n",
    "            pverbs.append(male_gaze['Patient Verbs'][j])\n",
    "            attrs.append(male_gaze['Total Attributes'][j])\n",
    "    plot_analysis['Wikipedia movie ID'] = movies\n",
    "    plot_analysis['Character_Name'] = chars\n",
    "    plot_analysis['Agent Verbs'] = averbs\n",
    "    plot_analysis['Patient Verbs'] = pverbs\n",
    "    plot_analysis['Attributes'] = attrs\n",
    "    return plot_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39b3225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a8735a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bb90a63aab466f8b4f679c9d957c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 15:35:20 INFO: Downloading default packages for language: en (English) ...\n",
      "2022-12-14 15:35:22 INFO: File exists: C:\\Users\\nerea\\stanza_resources\\en\\default.zip\n",
      "2022-12-14 15:35:26 INFO: Finished downloading models and saved to C:\\Users\\nerea\\stanza_resources.\n",
      "2022-12-14 15:35:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ca80572a8f44cba8970d030fadeb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 15:35:28 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-12-14 15:35:28 INFO: Use device: cpu\n",
      "2022-12-14 15:35:28 INFO: Loading: tokenize\n",
      "2022-12-14 15:35:28 INFO: Loading: pos\n",
      "2022-12-14 15:35:28 INFO: Loading: lemma\n",
      "2022-12-14 15:35:28 INFO: Loading: depparse\n",
      "2022-12-14 15:35:28 INFO: Loading: ner\n",
      "2022-12-14 15:35:29 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, pos, lemma, depparse, ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d55ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Jackson Maine, a famous country music singer privately battling an alcohol and drug addiction, plays a concert in California. His main support is Bobby, his manager and older half-brother. After the show, Jackson visits a drag bar where he witnesses a performance by Ally, a waitress and singer-songwriter. Jackson is amazed by her performance, and they spend the night speaking to each other, where Ally discloses to him the troubles she has faced in pursuing a professional music career. Jackson invites Ally to his next show. Despite her initial refusal she attends and, with Jackson's encouragement, sings on stage with him. Jackson invites Ally to go on tour with him, and they form a romantic relationship. In Arizona, Ally and Jackson visit the ranch where Jackson grew up and where his father is buried, only to discover that Bobby sold the land. Angered at his betrayal, Jackson punches Bobby, who subsequently quits as his manager. Before doing so, Bobby reveals that he did inform Jackson about the sale, but the latter was too inebriated to notice.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd1d545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jackson Maine, a famous country music singer privately battling an alcohol and drug addiction, plays a concert in California. His main support is Bobby, his manager and older half-brother. After the show, Jackson visits a drag bar where he witnesses a performance by Ally, a waitress and singer-songwriter. Jackson is amazed by her performance, and they spend the night speaking to each other, where Ally discloses to him the troubles she has faced in pursuing a professional music career. Jackson invites Ally to his next show. Despite her initial refusal she attends and, with Jackson's encouragement, sings on stage with him. Jackson invites Ally to go on tour with him, and they form a romantic relationship. In Arizona, Ally and Jackson visit the ranch where Jackson grew up and where his father is buried, only to discover that Bobby sold the land. Angered at his betrayal, Jackson punches Bobby, who subsequently quits as his manager. Before doing so, Bobby reveals that he did inform Jackson about the sale, but the latter was too inebriated to notice.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f60566f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "chars = get_characters(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a5556b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ally', 'Bobby', 'Jackson']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b63a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = char_attributes(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6be2f536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Character Names</th>\n",
       "      <th>Total Attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ally</td>\n",
       "      <td>performance waitress singer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Jackson</td>\n",
       "      <td>encouragement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index Character Names             Total Attributes\n",
       "0      0            Ally  performance waitress singer\n",
       "1      1         Jackson                encouragement"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5d5111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'singer': 'famous'}\n",
      "{'support': 'main', 'brother': 'older'}\n",
      "{}\n",
      "{'career': 'professional'}\n",
      "{}\n",
      "{'show': 'next'}\n",
      "{'refusal': 'initial'}\n",
      "{}\n",
      "{'relationship': 'romantic'}\n",
      "{}\n",
      "{}\n",
      "{'latter': 'inebriated'}\n"
     ]
    }
   ],
   "source": [
    "def recursive_find_adjs(root, sent):\n",
    "    children = [w for w in sent.words if w.head == root.id]\n",
    "\n",
    "    if not children:\n",
    "        return []\n",
    "\n",
    "    filtered_c = [w for w in children if w.deprel == \"conj\" and w.upos == \"ADJ\"]\n",
    "    # Do not include an adjective if it is the parent of a noun to prevent\n",
    "    results = [w for w in filtered_c if not any(sub.head == w.id and sub.upos == \"NOUN\" for sub in sent.words)]\n",
    "    for w in children:\n",
    "        results += recursive_find_adjs(w, sent)\n",
    "\n",
    "    return results\n",
    "\n",
    "for sent in doc.sentences:\n",
    "    nouns = [w for w in sent.words if w.upos == \"NOUN\"]\n",
    "    noun_adj_pairs = {}\n",
    "    for noun in nouns:\n",
    "        # Find constructions in the form of \"La voiture est belle\"\n",
    "        # In this scenario, the adjective is the parent of the noun\n",
    "        cop_root = sent.words[noun.head-1]\n",
    "        adjs = [cop_root] + recursive_find_adjs(cop_root, sent) if cop_root.upos == \"ADJ\" else []\n",
    "\n",
    "        # Find constructions in the form of \"La femme intelligente et belle\"\n",
    "        # Here, the adjectives are descendants of the noun\n",
    "        mod_adjs = [w for w in sent.words if w.head == noun.id and w.upos == \"ADJ\"]\n",
    "        # This should only be one element because conjunctions are hierarchical\n",
    "        if mod_adjs:\n",
    "            mod_adj = mod_adjs[0]\n",
    "            adjs.extend([mod_adj] + recursive_find_adjs(mod_adj, sent))\n",
    "\n",
    "        if adjs:\n",
    "            unique_adjs = []\n",
    "            unique_ids = set()\n",
    "            for adj in adjs:\n",
    "                if adj.id not in unique_ids:\n",
    "                    unique_adjs.append(adj)\n",
    "                    unique_ids.add(adj.id)\n",
    "\n",
    "            noun_adj_pairs[noun.text] = \" \".join([adj.text for adj in unique_adjs])\n",
    "\n",
    "    print(noun_adj_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "812a7c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heads_w = {}\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    heads = []\n",
    "    words = []\n",
    "    head_words = []\n",
    "    for word in sentence.words:\n",
    "        #print(word.text, word.head, word.deprel, word.pos)\n",
    "        heads.extend([word.head])\n",
    "        words.extend([word.text])\n",
    "    for i,h in enumerate(heads):\n",
    "        for word in sentence.words:\n",
    "            if word.id == h:\n",
    "                head_words.append(word.text) \n",
    "    for i,w in enumerate(words[0:-1]):\n",
    "        heads_w[words[i]] = head_words[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "96501143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method values of dict object at 0x000001D601FF9400>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b23cbbda",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neuralcoref'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mneuralcoref\u001b[39;00m\n\u001b[0;32m      3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_lg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m neuralcoref\u001b[38;5;241m.\u001b[39madd_to_pipe(nlp)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'neuralcoref'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e24aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6819866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
